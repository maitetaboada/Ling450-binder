{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ling 450/807 SFU - Assignment 1\n",
    "\n",
    "This assignment walks you through two different ways of extracting simple quotes from text and then directs you to a third, already implemented way. Your task is to enhance the simple methods or develop your own. For further instructions, check the assignment file on Canvas. \n",
    "The binder contains this notebook and some sample files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 1: Using regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loads and processes only one file at a time. You need to do 5 and comment on the results\n",
    "# to load the 5 texts, you can just change the name of the file below or figure out a way \n",
    "# to pass a list of files to the read command. It's up to you\n",
    "\n",
    "with open (\"data/5c1dbe1d1e67d78e2797d611.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sents(text):\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    doc = nlp(text)\n",
    "    sentences = list(doc.sents)\n",
    "    return(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding text within quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quotes(text):\n",
    "    quotes = re.findall(r' \"(.*?)\"', text)\n",
    "    return(quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "found_sents = find_sents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I have no idea where that information came from because both Clark and I were there in the office with all of the workers from the orphanage.']\n",
      "['the Government of Canada has obligations under international conventions to ensure children are not abducted, bought or sold, or removed from their biological families without legal consent.']\n",
      "['in some cases, extra steps in the citizenship or immigration process may be needed to make sure the adoption meets all requirements of international adoption.']\n"
     ]
    }
   ],
   "source": [
    "# note: this just prints the text in quotes. If you want to save it locally\n",
    "# to analyze how the 3 approaches are different, you need to run a command to save\n",
    "# for instance to a text file\n",
    "\n",
    "for sent in found_sents:\n",
    "    str_sent = str(sent)\n",
    "    found_quotes = get_quotes(str_sent)\n",
    "    if len(found_quotes) > 0:\n",
    "        print(found_quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 2: Using spaCy's Matcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach is based on notebooks by Dr. W.J.B. Mattingly, http://spacy.pythonhumanities.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all the stuff we'll need\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "matcher = Matcher(nlp.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy's Matcher\n",
    "This notebook relies on spaCy's Matcher (see Advanced NLP with spaCy, [chapter 2](https://course.spacy.io/en/chapter2)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding quotes and speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a text file. Remember, you have to do 5\n",
    "with open (\"data/5c1dbe1d1e67d78e2797d611.txt\", \"r\", encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert it to a spacy doc\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding proper nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61\n",
      "(3232560085755078826, 1, 2) CTV\n",
      "(3232560085755078826, 2, 3) Vancouver\n",
      "(3232560085755078826, 6, 7) Abbotsford\n",
      "(3232560085755078826, 8, 9) B.C.\n",
      "(3232560085755078826, 25, 26) Africa\n",
      "(3232560085755078826, 42, 43) Kim\n",
      "(3232560085755078826, 44, 45) Clark\n",
      "(3232560085755078826, 45, 46) Moran\n",
      "(3232560085755078826, 52, 53) Immigration\n",
      "(3232560085755078826, 54, 55) Refugees\n"
     ]
    }
   ],
   "source": [
    "# This is optional. It just tells you who are the people mentioned. You can use it later if you want to find out the speakers of the quotes\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_n = [{\"POS\": \"PROPN\"}]\n",
    "matcher.add(\"PROPER_NOUNS\", [pattern_n], greedy=\"LONGEST\")\n",
    "doc = nlp(text)\n",
    "matches = matcher(doc)\n",
    "print (len(matches))\n",
    "for match in matches[:10]:\n",
    "    print (match, doc[match[1]:match[2]])\n",
    "    \n",
    "## You can try to extract full names by adding multi-word nouns, http://spacy.pythonhumanities.com/02_02_matcher.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "(16432004385153140588, 115, 133) \"The fact that we are being accused right now of an unethical adoption is crazy.\"\n",
      "(16432004385153140588, 164, 174) \"It does say that in the letter,\"\n",
      "(16432004385153140588, 179, 209) \"I have no idea where that information came from because both Clark and I were there in the office with all of the workers from the orphanage.\"\n"
     ]
    }
   ],
   "source": [
    "# a simple pattern to extract things in single quotes\n",
    "# as with Approach 1, the for loop prints the results to the screen\n",
    "# you can try and save it to a file if you want to compare with Approach 1 and 3\n",
    "\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern_q = [{'ORTH': '\"'}, {'IS_ALPHA': True, \"OP\": \"+\"}, {'IS_PUNCT': True, \"OP\": \"*\"}, {'ORTH': '\"'}]\n",
    "matcher.add(\"QUOTES\", [pattern_q], greedy='LONGEST')\n",
    "doc = nlp(text)\n",
    "matches_q = matcher(doc)\n",
    "matches_q.sort(key = lambda x: x[1])\n",
    "print (len(matches_q))\n",
    "for match in matches_q[:10]:\n",
    "    print (match, doc[match[1]:match[2]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach 3: Implemented version\n",
    "This approach was implemented by colleagues at the [Australian Text Analytics Platform](https://www.atap.edu.au/) (ATAP). The approach is based on the [Gender Gap Tracker](https://github.com/sfu-discourse-lab/GenderGapTracker) done in the Discourse Processing Lab here at SFU. \n",
    "\n",
    "The first link below leads you to a binder where you can load your own files and download the output. If you prefer to do everything in your own notebook, you can download/clone the project and you'll see a notebook there (quote_extractor_notebook.ipynb)\n",
    "\n",
    "* [Binder link](https://github.com/Australian-Text-Analytics-Platform/quotation-tool/blob/workshop_01_20220908/README.md)\n",
    "* [Regular GitHub project](https://github.com/Australian-Text-Analytics-Platform/quotation-tool)\n",
    "\n",
    "Within the ATAP binder, upload 5 files from A1/data (the same you did for approaches 1 and 2), process them and download the results to your own computer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn\n",
    "\n",
    "Check instructions on Canvas for what to do and what to submit. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
